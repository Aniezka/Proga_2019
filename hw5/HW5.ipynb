{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import flask\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from flask_sqlalchemy import SQLAlchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.dialects.postgresql import JSONB, ARRAY, JSON\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import Table, ForeignKey, Text\n",
    "from sqlalchemy import create_engine, Column, Integer, MetaData\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy.orm import relationship, backref, sessionmaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.txt\", \"r\") as f:\n",
    "    DATABASE = eval(f.read())\n",
    "\n",
    "engine = create_engine(URL(**DATABASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Models(Base):\n",
    "    __tablename__ = 'models'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    model = Column(JSONB)\n",
    "    cv_results = Column(JSONB)\n",
    "    status = Column(Text)\n",
    "    datasets_id = Column(\n",
    "        Integer,\n",
    "        ForeignKey('datasets.id'), unique=True\n",
    "    )\n",
    "    datasets = relationship('Datasets',\n",
    "                            backref=backref('models', uselist=False))\n",
    "\n",
    "\n",
    "class Datasets(Base):\n",
    "    __tablename__ = 'datasets'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    data = Column(Text)\n",
    "    target = Column(Text)\n",
    "    n_folds = Column(Text)\n",
    "    fit_intercept = Column(Text)\n",
    "    l2_coef = Column(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.metadata.create_all(engine)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backend methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel db-checker-trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def check_models_db():\n",
    "    for model in session.query(Models):\n",
    "        if model.status == \"new\":\n",
    "            m_str = \"'check_model_db' find new model with id: \"\n",
    "            logging.debug(m_str + str(model.id))\n",
    "            new_model = Models(id=model.id,\n",
    "                               status=\"train\")\n",
    "            session.query(Models)\\\n",
    "                .filter_by(id=model.id)\\\n",
    "                .update({\"status\": \"train\"})\n",
    "            session.commit()\n",
    "            m_str = \"script set 'train' status for model with id: \"\n",
    "            logging.debug(m_str + str(model.id))\n",
    "            X_train, Y_train = get_train_data(model.datasets.data,\n",
    "                                              model.datasets.target)\n",
    "            fit_inter = eval(model.datasets.fit_intercept)\n",
    "            n_folds = eval(model.datasets.n_folds)\n",
    "            alpha_arr = eval(model.datasets.l2_coef)\n",
    "            res = train_kfold(X_train,\n",
    "                              Y_train,\n",
    "                              k_folds=n_folds,\n",
    "                              fit_intercept=fit_inter,\n",
    "                              alpha_arr=alpha_arr)\n",
    "            session.query(Models)\\\n",
    "                .filter_by(id=model.id)\\\n",
    "                .update({\"model\": res[\"model\"],\n",
    "                         \"cv_results\": res[\"cv_results\"],\n",
    "                         \"status\": \"ready\"})\n",
    "            m_str = \"script set 'ready' status for model with id: \"\n",
    "            logging.debug(m_str + str(model.id))\n",
    "            session.commit()\n",
    "\n",
    "\n",
    "def train_script():\n",
    "    while True:\n",
    "        sleep(10)\n",
    "        check_models_db()\n",
    "        m_str = \"'train_script' check new models once in 10 sec.\"\n",
    "        logging.debug(m_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "\n",
    "def data_checker(data, target):\n",
    "    data = StringIO(data)\n",
    "    logging.debug(\"'data_cheker' working\\n\")\n",
    "    try:\n",
    "        df = pd.read_csv(data, sep=\",\")\n",
    "    except Exception:\n",
    "        return \"'read_csv' end with error\"\n",
    "    if target not in df.columns:\n",
    "        return \"'target' column noi in data\"\n",
    "    if df.isnull().values.any():\n",
    "        return \"'data' contain NaN\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn import datasets\n",
    "import sklearn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def get_free_id():\n",
    "    count = 1\n",
    "    idx_arr = [i[0] for i in session.query(Models.id).all()]\n",
    "    while count in idx_arr:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def get_train_data(data, target):\n",
    "    data = StringIO(data)\n",
    "    df = pd.read_csv(data, sep=\",\")\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    y_train = df[target].to_frame()\n",
    "    X_train = df.drop(target, axis=1)\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def predict_from_model(data, model_db):\n",
    "    m_str = \"'predict_from_model' with id: \"\n",
    "    logging.debug(m_str + str(model_db.id))\n",
    "    data = StringIO(data)\n",
    "    df = pd.read_csv(data, sep=\",\")\n",
    "    X_predict = df.reindex(sorted(df.columns), axis=1)\n",
    "    fit_inter = eval(model_db.datasets.fit_intercept)\n",
    "\n",
    "    mdl = Ridge()\n",
    "    if fit_inter:\n",
    "        mdl.intercept_ = model_db.model[\"intercept\"]\n",
    "    coef_arr = []\n",
    "    m_str = \"'predict_from_model' with id: \"\n",
    "    logging.debug(m_str + str(model_db.model))\n",
    "    for feature in X_predict.columns:\n",
    "        for name_coef, val in model_db.model[\"coef\"].items():\n",
    "            if feature == name_coef:\n",
    "                coef_arr.append(val)\n",
    "    if len(coef_arr) != len(X_predict.columns):\n",
    "        m_str = \"data for prediction is not valid for this model\"\n",
    "        return {\"error\": m_str}\n",
    "    if df.isnull().values.any():\n",
    "        return {\"error\": \"'data' contain NaN\"}\n",
    "    mdl.coef_ = np.array(coef_arr)\n",
    "    res = mdl.predict(X_predict)\n",
    "    m_str = \"end 'predict_from_model' with id: \"\n",
    "    logging.debug(m_str + str(model_db.id))\n",
    "    return {\"result\": str(res)}\n",
    "\n",
    "\n",
    "def train_kfold(X_train_df,\n",
    "                y_train_df,\n",
    "                k_folds=2,\n",
    "                fit_intercept=True,\n",
    "                alpha_arr=[1, 0.2, 0.1, 0.01, 0.001]):\n",
    "    logging.debug(\"'train_kfold' function is progress...\")\n",
    "    X_train, y_train = np.array(X_train_df), np.array(y_train_df)\n",
    "    parameters = [{'fit_intercept': [fit_intercept],\n",
    "                   'alpha': alpha_arr}]\n",
    "    clf = GridSearchCV(Ridge(),\n",
    "                       parameters,\n",
    "                       cv=k_folds,\n",
    "                       scoring=make_scorer(\n",
    "                           mean_squared_error,\n",
    "                           greater_is_better=False))\n",
    "    clf.fit(X_train, y_train)\n",
    "    best_model = clf.best_estimator_\n",
    "    result_json = {}\n",
    "    result_json[\"model\"] = {}\n",
    "    result_json[\"model\"][\"intercept\"] = best_model.intercept_[0]\n",
    "    result_json[\"model\"][\"coef\"] = {}\n",
    "    for ind, coef in enumerate(best_model.coef_[0]):\n",
    "        ind_buf = X_train_df.columns[ind]\n",
    "        result_json[\"model\"][\"coef\"][ind_buf] = coef\n",
    "    result_json[\"cv_results\"] = {}\n",
    "    for ind, alp in enumerate(alpha_arr):\n",
    "        m_dict = {\n",
    "            \"mean_mse\": -clf.cv_results_['mean_test_score'][ind]\n",
    "        }\n",
    "        result_json[\"cv_results\"][alp] = m_dict\n",
    "    logging.debug(\"'train_kfold' function end\")\n",
    "    return result_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flask Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### set logger parameters to level=DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:test\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "DEBUG:root:'train_script' check new models once in 10 sec.\n",
      "INFO:werkzeug:127.0.0.1 - - [01/May/2020 15:18:15] \"\u001b[37mGET /hello HTTP/1.1\u001b[0m\" 200 -\n",
      "DEBUG:root:'GET' branch in app/train\n",
      "INFO:werkzeug:127.0.0.1 - - [01/May/2020 15:18:15] \"\u001b[37mGET /train HTTP/1.1\u001b[0m\" 200 -\n",
      "DEBUG:root:'POST' branch in app/train\n",
      "DEBUG:root:'data_cheker' working\n",
      "\n",
      "DEBUG:root:'POST' commit after checker\n",
      "INFO:werkzeug:127.0.0.1 - - [01/May/2020 15:18:16] \"\u001b[37mPOST /train HTTP/1.1\u001b[0m\" 200 -\n",
      "DEBUG:root:'check_model_db' find new model with id: 11\n",
      "DEBUG:root:script set 'train' status for model with id: 11\n",
      "DEBUG:root:'train_kfold' function is progress...\n",
      "DEBUG:root:'train_kfold' function end\n",
      "DEBUG:root:script set 'ready' status for model with id: 11\n",
      "DEBUG:root:'train_script' check new models once in 10 sec.\n",
      "DEBUG:root:'GET' branch in model/<id>\n",
      "INFO:werkzeug:127.0.0.1 - - [01/May/2020 15:18:20] \"\u001b[37mGET /model/11 HTTP/1.1\u001b[0m\" 200 -\n",
      "DEBUG:root:'POST' branch in /model/<id>/predict\n",
      "DEBUG:root:model with id : 11 in DB\n",
      "DEBUG:root:'predict_from_model' with id: 11\n",
      "DEBUG:root:'predict_from_model' with id: {'coef': {'b': 0.0, 'c': 0.9241024087259506}, 'intercept': 0.45538554764429584}\n",
      "DEBUG:root:end 'predict_from_model' with id: 11\n",
      "INFO:werkzeug:127.0.0.1 - - [01/May/2020 15:18:21] \"\u001b[37mPOST /model/11/predict HTTP/1.1\u001b[0m\" 200 -\n",
      "DEBUG:root:'POST' branch in app/train\n",
      "DEBUG:root:'data_cheker' working\n",
      "\n",
      "ERROR:root:'data_checker' end with error\n",
      "INFO:werkzeug:127.0.0.1 - - [01/May/2020 15:18:23] \"\u001b[37mPOST /train HTTP/1.1\u001b[0m\" 200 -\n",
      "DEBUG:root:'POST' branch in app/train\n",
      "DEBUG:root:'data_cheker' working\n",
      "\n",
      "ERROR:root:'data_checker' end with error\n",
      "INFO:werkzeug:127.0.0.1 - - [01/May/2020 15:18:24] \"\u001b[37mPOST /train HTTP/1.1\u001b[0m\" 200 -\n",
      "DEBUG:root:'POST' branch in app/train\n",
      "DEBUG:root:'data_cheker' working\n",
      "\n",
      "DEBUG:root:'POST' commit after checker\n",
      "INFO:werkzeug:127.0.0.1 - - [01/May/2020 15:18:25] \"\u001b[37mPOST /train HTTP/1.1\u001b[0m\" 200 -\n",
      "DEBUG:root:'POST' branch in /model/<id>/predict\n",
      "DEBUG:root:model with id : 12 in DB\n",
      "INFO:werkzeug:127.0.0.1 - - [01/May/2020 15:18:25] \"\u001b[37mPOST /model/12/predict HTTP/1.1\u001b[0m\" 200 -\n",
      "DEBUG:root:'GET' branch in model/<id>\n",
      "INFO:werkzeug:127.0.0.1 - - [01/May/2020 15:18:26] \"\u001b[37mGET /model/10000 HTTP/1.1\u001b[0m\" 200 -\n",
      "DEBUG:root:'check_model_db' find new model with id: 12\n",
      "DEBUG:root:script set 'train' status for model with id: 12\n",
      "DEBUG:root:'train_kfold' function is progress...\n",
      "DEBUG:root:'train_kfold' function end\n",
      "DEBUG:root:script set 'ready' status for model with id: 12\n",
      "DEBUG:root:'train_script' check new models once in 10 sec.\n",
      "DEBUG:root:'POST' branch in /model/<id>/predict\n",
      "DEBUG:root:model with id : 4 in DB\n",
      "DEBUG:root:'predict_from_model' with id: 4\n",
      "DEBUG:root:'predict_from_model' with id: {'coef': {'b': 0.0, 'c': 0.9241024087259506}, 'intercept': 0.45538554764429584}\n",
      "INFO:werkzeug:127.0.0.1 - - [01/May/2020 15:18:27] \"\u001b[37mPOST /model/4/predict HTTP/1.1\u001b[0m\" 200 -\n",
      "DEBUG:root:'train_script' check new models once in 10 sec.\n",
      "DEBUG:root:'train_script' check new models once in 10 sec.\n",
      "DEBUG:root:'train_script' check new models once in 10 sec.\n",
      "DEBUG:root:'train_script' check new models once in 10 sec.\n",
      "DEBUG:root:'train_script' check new models once in 10 sec.\n",
      "DEBUG:root:'train_script' check new models once in 10 sec.\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, jsonify, request, render_template\n",
    "import threading\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "@app.route('/model/')\n",
    "@app.route('/model/<id>', methods=['GET'])\n",
    "def model_info(id=None):\n",
    "    logging.debug(\"'GET' branch in model/<id>\")\n",
    "    model_db = session.query(Models).get(id)\n",
    "    if model_db is not None:\n",
    "        return jsonify(response={\"model\": model_db.model,\n",
    "                                 \"cv_results\": model_db.cv_results})\n",
    "    return jsonify(response={\"error\": \"Not valid model_id\"})\n",
    "\n",
    "\n",
    "@app.route('/model/<id>/predict', methods=['GET', 'POST'])\n",
    "def model(id=None):\n",
    "    if request.method == 'GET':\n",
    "        logging.debug(\"'GET' branch in /model/<id>/predict\")\n",
    "        m_dict = {\"'/model/<id>/predict' status\": \"work\"}\n",
    "        return jsonify(response = m_dict)\n",
    "    if request.method == 'POST':\n",
    "        logging.debug(\"'POST' branch in /model/<id>/predict\")\n",
    "        model_db = session.query(Models).get(id)\n",
    "        if model_db is not None:\n",
    "            m_str = \"model with id : {} in DB\".format(model_db.id)\n",
    "            logging.debug(m_str)\n",
    "            if model_db.status == \"train\":\n",
    "                m_dict = {\"error\":\n",
    "                          \"model already in training stage\"}\n",
    "                return jsonify(response=m_dict)\n",
    "            if model_db.status == \"new\":\n",
    "                m_dict = {\"error\":\n",
    "                          \"model in line for training\"}\n",
    "                return jsonify(response=m_dict)\n",
    "            else:\n",
    "                res = predict_from_model(\n",
    "                    request.form[\"data\"],\n",
    "                    model_db\n",
    "                )\n",
    "        return jsonify(response=res)\n",
    "    return jsonify(response={\"error\": \"Not valid model_id\"})\n",
    "\n",
    "\n",
    "@app.route('/hello', methods=['GET'])\n",
    "def hello_world():\n",
    "    arr = {}\n",
    "    arr[\"blah\"] = []\n",
    "    arr[\"blah\"].append(\"stuff\")\n",
    "    return jsonify(response=arr)\n",
    "\n",
    "\n",
    "@app.route('/train', methods=['GET', 'POST'])\n",
    "def train():\n",
    "    if request.method == 'GET':\n",
    "        logging.debug(\"'GET' branch in app/train\")\n",
    "        return jsonify(response={\"status\": \"work\"})\n",
    "    if request.method == 'POST':\n",
    "        logging.debug(\"'POST' branch in app/train\")\n",
    "        error = data_checker(\n",
    "            request.form[\"data\"],\n",
    "            request.form[\"target\"]\n",
    "        )\n",
    "        if error != None:\n",
    "            logging.error(\"'data_checker' end with error\")\n",
    "            return jsonify(response={'error': error})\n",
    "        model_id = get_free_id()\n",
    "        \n",
    "        d_obj = Datasets(id=model_id,\n",
    "                         data=request.form[\"data\"],\n",
    "                         target=request.form[\"target\"],\n",
    "                         n_folds=str(request.form[\"n_folds\"]),\n",
    "                         fit_intercept=str(\n",
    "                             request.form[\"fit_intercept\"]\n",
    "                         ),\n",
    "                         l2_coef=str(request.form[\"l2_coef\"]))\n",
    "        new_model = Models(id=model_id,\n",
    "                           model={},\n",
    "                           cv_results={},\n",
    "                           status=\"new\",\n",
    "                           datasets_id=model_id,\n",
    "                           datasets=d_obj)\n",
    "        session.add(new_model)\n",
    "        session.commit()\n",
    "        logging.debug(\"'POST' commit after checker\")\n",
    "    return jsonify(response={\"model_id\": model_id})\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    p = Process(target=train_script)\n",
    "    p.start()\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
